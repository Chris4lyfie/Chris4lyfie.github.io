<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Proxmox Infrastructure Reliability</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      line-height: 1.6;
      color: #2c3e50;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      padding: 2rem 1rem;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      background: white;
      border-radius: 12px;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      overflow: hidden;
    }

    header {
      background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
      color: white;
      padding: 3rem 2rem;
      text-align: center;
    }

    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      letter-spacing: -0.5px;
    }

    .subtitle {
      font-size: 1.1rem;
      opacity: 0.9;
      font-weight: 300;
    }

    .content {
      padding: 2rem;
    }

    h2 {
      color: #1e3c72;
      font-size: 1.75rem;
      margin: 2.5rem 0 1rem;
      padding-bottom: 0.5rem;
      border-bottom: 3px solid #667eea;
      display: inline-block;
    }

    .content > h2:first-child {
      margin-top: 0;
    }

    p {
      margin-bottom: 1rem;
      color: #4a5568;
    }

    strong {
      color: #2c3e50;
      font-weight: 600;
    }

    ul {
      margin: 1rem 0 1.5rem 1.5rem;
      list-style: none;
    }

    ul li {
      position: relative;
      padding-left: 1.5rem;
      margin-bottom: 0.5rem;
      color: #4a5568;
    }

    ul li:before {
      content: "â–¹";
      position: absolute;
      left: 0;
      color: #667eea;
      font-weight: bold;
    }

    .diagram-container {
      margin: 2rem 0;
      text-align: center;
      background: #f7fafc;
      padding: 2rem;
      border-radius: 8px;
      border: 1px solid #e2e8f0;
    }

    .diagram-container img {
      max-width: 100%;
      height: auto;
      border-radius: 6px;
    }

    .failure-event {
      background: #fff5f5;
      border-left: 4px solid #f56565;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 0 8px 8px 0;
    }

    .failure-event h2 {
      color: #c53030;
      border-bottom-color: #f56565;
      margin-top: 0;
    }

    .architectural-change {
      background: #f0fff4;
      border-left: 4px solid #48bb78;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 0 8px 8px 0;
    }

    .architectural-change h2 {
      color: #2f855a;
      border-bottom-color: #48bb78;
      margin-top: 0;
    }

    .principles {
      background: #ebf8ff;
      border-left: 4px solid #4299e1;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 0 8px 8px 0;
    }

    .principles h2 {
      color: #2c5282;
      border-bottom-color: #4299e1;
      margin-top: 0;
    }

    .badge {
      display: inline-block;
      padding: 0.25rem 0.75rem;
      background: #667eea;
      color: white;
      border-radius: 12px;
      font-size: 0.875rem;
      font-weight: 500;
      margin-right: 0.5rem;
    }

    .impact-high {
      background: #f56565;
    }

    @media (max-width: 768px) {
      body {
        padding: 1rem 0.5rem;
      }

      h1 {
        font-size: 2rem;
      }

      .content {
        padding: 1.5rem;
      }

      h2 {
        font-size: 1.5rem;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>Homelab Infrastructure Reliability</h1>
      <p class="subtitle">Case Study: Proxmox Virtualization Platform</p>
    </header>

    <div class="content">
      <h2>System Overview</h2>
      
      <div class="diagram-container">
        <img src="../assets/diagrams/network-architecture.png"
             alt="Home infrastructure architecture showing router, switch, hypervisor, storage, and management services">
      </div>
      
      <p>
        Primary self-managed virtualization and infrastructure platform supporting storage,
        network services, and application workloads. Designed with VLAN-based segmentation,
        out-of-band management, and fault containment as core reliability goals.
      </p>

      <h2>Bring-Up and Initial Risk Assessment</h2>
      <p>
        Initial system bring-up focused on network reachability and remote manageability.
        The host had no direct video output, requiring full reliance on IPMI for access and recovery.
        Primary concern was compatibility between dual 10 Gb NICs on the server and a 1 Gb network
        edge, with risk of total loss of connectivity during initial configuration.
      </p>

      <p>Before trusting the system for unattended operation, the following were verified:</p>
      <ul>
        <li>BIOS thermal and power configuration</li>
        <li>Correct ECC memory detection, capacity, and speed</li>
        <li>IPMI network reachability and remote power cycling</li>
        <li>VLAN reachability on the management network</li>
      </ul>

      <p>
        No instability was observed on first boot. However, a subsequent BIOS management
        misconfiguration temporarily locked out IPMI access, reinforcing the need for
        conservative change control on management paths.
      </p>

      <div class="failure-event">
        <h2>Failure Event 1: VLAN Reset Causing Management Lockout</h2>

        <p><strong>Symptom:</strong> Loss of access to wireless access point web management interface.</p>
        <p><span class="badge impact-high">Impact: High</span> Wireless configuration and recovery were blocked.</p>

        <p>
          The issue was detected immediately after a firmware update when the access point
          became unreachable on the management VLAN.
        </p>

        <p><strong>Root Cause:</strong> Firmware update reset the management VLAN configuration on the access point.</p>

        <p><strong>Recovery Path:</strong></p>
        <ul>
          <li>Configured static IP on a laptop within the expected management subnet</li>
          <li>Established direct Ethernet connection to the access point</li>
          <li>Reconfigured management VLAN and verified persistence post-update</li>
        </ul>

        <p>
          Total lockout duration was approximately two hours. Without physical access, recovery
          would not have been possible.
        </p>

        <p><strong>Permanent Change:</strong></p>
        <ul>
          <li>Dedicated management VLAN enforced</li>
          <li>Post-update configuration verification added as a required step</li>
        </ul>
      </div>

      <div class="failure-event">
        <h2>Failure Event 2: Cross-VLAN DNS Isolation (Pi-hole)</h2>

        <p><strong>Symptom:</strong> Devices on non-management VLANs lost internet access when configured to use Pi-hole for DNS.</p>
        <p><span class="badge impact-high">Impact: High</span> Entire VLANs experienced loss of connectivity.</p>

        <p>
          The issue manifested only after redirecting DNS traffic from other VLANs to the Pi-hole instance.
          Devices could route to the internet until DNS resolution failed.
        </p>

        <p><strong>Root Cause:</strong> Firewall rules on the ER605v2 router did not permit cross-VLAN DNS traffic.</p>

        <p><strong>Recovery Path:</strong></p>
        <ul>
          <li>Validated that routing and NAT were functional</li>
          <li>Identified DNS as the sole failing service</li>
          <li>Implemented firewall rules explicitly allowing DNS traffic only</li>
        </ul>

        <p><strong>Permanent Change:</strong></p>
        <ul>
          <li>Moved Pi-hole permanently to the management VLAN</li>
          <li>Restricted cross-VLAN access to DNS traffic only</li>
        </ul>
      </div>

      <div class="failure-event">
        <h2>Failure Event 3: Network Trunk Misconfiguration</h2>

        <p><strong>Symptom:</strong> Complete loss of access to network switch and access point.</p>
        <p><span class="badge impact-high">Impact: High</span> All network traffic disrupted.</p>

        <p>
          The failure occurred after simultaneous changes to tagged and untagged VLAN settings
          across all switch ports.
        </p>

        <p><strong>Root Cause:</strong> Incorrect trunk configuration combined with modifying all ports at once,
        eliminating the management path.</p>

        <p><strong>Recovery Path:</strong></p>
        <ul>
          <li>Performed factory reset of the managed switch</li>
          <li>Rebuilt VLAN configuration incrementally</li>
          <li>Restored management access before reintroducing trunks</li>
        </ul>

        <p>Total recovery time was approximately two hours.</p>

        <p><strong>Permanent Change:</strong></p>
        <ul>
          <li>One switch port permanently reserved as management-only</li>
          <li>All VLAN changes performed incrementally, one port at a time</li>
          <li>Physical port-to-VLAN documentation maintained</li>
        </ul>
      </div>

      <div class="architectural-change">
        <h2>Architectural Change: TrueNAS VM to Bare Metal</h2>

        <p>
          TrueNAS was initially deployed as a virtual machine under Proxmox to evaluate SATA
          passthrough feasibility. This design introduced unacceptable reliability risks.
        </p>

        <p><strong>Identified Risks:</strong></p>
        <ul>
          <li>Data availability tied to hypervisor uptime</li>
          <li>Resource contention during concurrent workloads</li>
          <li>Inability to perform SMART monitoring within a VM</li>
          <li>Risk of catastrophic data loss from accidental VM deletion</li>
        </ul>

        <p>
          The system was migrated to a dedicated bare-metal server to isolate storage from
          compute workloads.
        </p>

        <p><strong>Reliability Improvements:</strong></p>
        <ul>
          <li>Eliminated resource contention from application workloads</li>
          <li>Restored direct disk health monitoring</li>
          <li>Decoupled data integrity from hypervisor stability</li>
        </ul>

        <p>
          Migration introduced physical risks during drive transfer and validation on older hardware,
          which were mitigated through careful sequencing and verification of pools and datasets.
        </p>
      </div>

      <div class="principles">
        <h2>Reliability Principles Applied</h2>
        <ul>
          <li>Always preserve a recovery path before making changes</li>
          <li>Management access must be isolated and minimally modified</li>
          <li>Changes are applied incrementally to avoid systemic lockout</li>
          <li>Critical data systems must not depend on non-critical services</li>
        </ul>
      </div>
    </div>
  </div>
</body>
</html>
