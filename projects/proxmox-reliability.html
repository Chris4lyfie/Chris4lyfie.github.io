<!DOCTYPE html>
<html>
<head>
  <title>Proxmox Infrastructure Reliability</title>
</head>
<body>

<h1>Proxmox Infrastructure Reliability Case Study</h1>

<h2>System Overview</h2>
<p>
Primary self-managed virtualization and infrastructure platform supporting storage,
network services, and application workloads. Designed with VLAN-based segmentation,
out-of-band management, and fault containment as core reliability goals.
</p>

<h2>Bring-Up and Initial Risk Assessment</h2>
<p>
Initial system bring-up focused on network reachability and remote manageability.
The host had no direct video output, requiring full reliance on IPMI for access and recovery.
Primary concern was compatibility between dual 10 Gb NICs on the server and a 1 Gb network
edge, with risk of total loss of connectivity during initial configuration.
</p>

<p>
Before trusting the system for unattended operation, the following were verified:
</p>
<ul>
  <li>BIOS thermal and power configuration</li>
  <li>Correct ECC memory detection, capacity, and speed</li>
  <li>IPMI network reachability and remote power cycling</li>
  <li>VLAN reachability on the management network</li>
</ul>

<p>
No instability was observed on first boot. However, a subsequent BIOS management
misconfiguration temporarily locked out IPMI access, reinforcing the need for
conservative change control on management paths.
</p>

<h2>Failure Event 1: VLAN Reset Causing Management Lockout</h2>

<p><strong>Symptom:</strong> Loss of access to wireless access point web management interface.</p>
<p><strong>Impact:</strong> High. Wireless configuration and recovery were blocked.</p>

<p>
The issue was detected immediately after a firmware update when the access point
became unreachable on the management VLAN.
</p>

<p><strong>Root Cause:</strong> Firmware update reset the management VLAN configuration on the access point.</p>

<p><strong>Recovery Path:</strong></p>
<ul>
  <li>Configured static IP on a laptop within the expected management subnet</li>
  <li>Established direct Ethernet connection to the access point</li>
  <li>Reconfigured management VLAN and verified persistence post-update</li>
</ul>

<p>
Total lockout duration was approximately two hours. Without physical access, recovery
would not have been possible.
</p>

<p><strong>Permanent Change:</strong></p>
<ul>
  <li>Dedicated management VLAN enforced</li>
  <li>Post-update configuration verification added as a required step</li>
</ul>

<h2>Failure Event 2: Cross-VLAN DNS Isolation (Pi-hole)</h2>

<p><strong>Symptom:</strong> Devices on non-management VLANs lost internet access when configured to use Pi-hole for DNS.</p>
<p><strong>Impact:</strong> High. Entire VLANs experienced loss of connectivity.</p>

<p>
The issue manifested only after redirecting DNS traffic from other VLANs to the Pi-hole instance.
Devices could route to the internet until DNS resolution failed.
</p>

<p><strong>Root Cause:</strong> Firewall rules on the ER605v2 router did not permit cross-VLAN DNS traffic.</p>

<p><strong>Recovery Path:</strong></p>
<ul>
  <li>Validated that routing and NAT were functional</li>
  <li>Identified DNS as the sole failing service</li>
  <li>Implemented firewall rules explicitly allowing DNS traffic only</li>
</ul>

<p><strong>Permanent Change:</strong></p>
<ul>
  <li>Moved Pi-hole permanently to the management VLAN</li>
  <li>Restricted cross-VLAN access to DNS traffic only</li>
</ul>

<h2>Failure Event 3: Network Trunk Misconfiguration</h2>

<p><strong>Symptom:</strong> Complete loss of access to network switch and access point.</p>
<p><strong>Impact:</strong> High. All network traffic disrupted.</p>

<p>
The failure occurred after simultaneous changes to tagged and untagged VLAN settings
across all switch ports.
</p>

<p><strong>Root Cause:</strong> Incorrect trunk configuration combined with modifying all ports at once,
eliminating the management path.
</p>

<p><strong>Recovery Path:</strong></p>
<ul>
  <li>Performed factory reset of the managed switch</li>
  <li>Rebuilt VLAN configuration incrementally</li>
  <li>Restored management access before reintroducing trunks</li>
</ul>

<p>
Total recovery time was approximately two hours.
</p>

<p><strong>Permanent Change:</strong></p>
<ul>
  <li>One switch port permanently reserved as management-only</li>
  <li>All VLAN changes performed incrementally, one port at a time</li>
  <li>Physical port-to-VLAN documentation maintained</li>
</ul>

<h2>Architectural Change: TrueNAS VM to Bare Metal</h2>

<p>
TrueNAS was initially deployed as a virtual machine under Proxmox to evaluate SATA
passthrough feasibility. This design introduced unacceptable reliability risks.
</p>

<p><strong>Identified Risks:</strong></p>
<ul>
  <li>Data availability tied to hypervisor uptime</li>
  <li>Resource contention during concurrent workloads</li>
  <li>Inability to perform SMART monitoring within a VM</li>
  <li>Risk of catastrophic data loss from accidental VM deletion</li>
</ul>

<p>
The system was migrated to a dedicated bare-metal server to isolate storage from
compute workloads.
</p>

<p><strong>Reliability Improvements:</strong></p>
<ul>
  <li>Eliminated resource contention from application workloads</li>
  <li>Restored direct disk health monitoring</li>
  <li>Decoupled data integrity from hypervisor stability</li>
</ul>

<p>
Migration introduced physical risks during drive transfer and validation on older hardware,
which were mitigated through careful sequencing and verification of pools and datasets.
</p>

<h2>Reliability Principles Applied</h2>
<ul>
  <li>Always preserve a recovery path before making changes</li>
  <li>Management access must be isolated and minimally modified</li>
  <li>Changes are applied incrementally to avoid systemic lockout</li>
  <li>Critical data systems must not depend on non-critical services</li>
</ul>

</body>
</html>

